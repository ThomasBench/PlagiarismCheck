{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib2to3.pgen2.tokenize import tokenize\n",
    "import nltk \n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import enchant \n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "import pyspark\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "eng_dict = enchant.Dict(\"en\")\n",
    "\n",
    "def to_ngram(index_tup):\n",
    "    return tuple([tup[1] for tup in index_tup])\n",
    "\n",
    "def correct_token(token: str) -> str : \n",
    "    if eng_dict.check(token):\n",
    "        return token\n",
    "    else:\n",
    "        suggestions = eng_dict.suggest(token)\n",
    "        if len(suggestions) > 0:\n",
    "            return suggestions[0]\n",
    "        return token\n",
    "\n",
    "def generate_n_gram(text: List[str], n: int) -> List[List[str]]:\n",
    "    return zip(*[text[i:] for i in range(n)])\n",
    "\n",
    "def treat_article(article_path:str, context, stopwords):\n",
    "    with open(article_path, mode = \"r\", encoding = \"utf-8\") as f:\n",
    "        data = ''.join(f.readlines())\n",
    "    full_article = ''.join([c for c in data if c.isalnum() or c == \" \"])\n",
    "    tokenized_article = list(enumerate(nltk.word_tokenize(full_article)))\n",
    "    filtered_article = [(index,token.lower()) for index,token in tokenized_article if token not in stopwords ]\n",
    "    filtered_indexes = [index for index,_ in filtered_article]\n",
    "    corrected_article = context.parallelize(filtered_article).map(lambda x: x[1]).map(correct_token).collect()\n",
    "    corrected_article = list(zip(filtered_indexes, corrected_article))\n",
    "    return tokenized_article, list(generate_n_gram(corrected_article, 4))\n",
    "\n",
    "def treat_article2(article_path:str, context, stopwords):\n",
    "    with open(article_path, mode = \"r\", encoding = \"utf-8\") as f:\n",
    "        data = ''.join(f.readlines())\n",
    "    full_article = ''.join([c for c in data if c.isalnum() or c == \" \"])\n",
    "    tokenized_article = list(enumerate(nltk.word_tokenize(full_article)))\n",
    "    filtered_article = [(index,token.lower()) for index,token in tokenized_article if token not in stopwords ]\n",
    "    filtered_indexes = [index for index,_ in filtered_article]\n",
    "    corrected_article = context.parallelize(filtered_article).map(lambda x: x[1]).map(correct_token).collect()\n",
    "    corrected_article = list(zip(filtered_indexes, corrected_article))\n",
    "    n_grams = list(generate_n_gram(corrected_article, 3))\n",
    "    n_gram_dict =defaultdict(list)\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_dict[to_ngram(n_gram)].append(n_gram[0][0])\n",
    "    return tokenized_article, n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_path = \"./txt files/french.txt\"\n",
    "en_path = \"./txt files/english.txt\"\n",
    "it_path = \"./txt files/italian.txt\"\n",
    "es_path = \"./txt files/spanish.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "treated_fr, fr_grams = treat_article2(fr_path, sc, eng_stopwords)\n",
    "treated_it, it_grams = treat_article2(it_path,sc,eng_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_sequences(art_grams_1,art_grams_2):\n",
    "    matching_grams = []\n",
    "    for gram_1 in art_grams_1:\n",
    "        for gram_2 in art_grams_2:\n",
    "            if gram_1 == gram_2:\n",
    "                art_1_matching_id = art_grams_1[gram_1].pop(0)\n",
    "                art_2_matching_id = art_grams_2[gram_1].pop(0)\n",
    "                matching_grams.append((gram_1,art_1_matching_id,art_2_matching_id))\n",
    "    return matching_grams\n",
    "\n",
    "def glue_sequence(sequence, gap_tolerance):\n",
    "    final = []\n",
    "    temp = [sequence[0][1:]]\n",
    "    last_seen = sequence[0][1:]\n",
    "    for _, ind_1, ind_2 in sequence[1:]:\n",
    "        # print(n_gram)\n",
    "        if 0<= ind_1 - last_seen[0] < gap_tolerance and 0<= ind_2 - last_seen[1] < gap_tolerance :\n",
    "            temp.append((ind_1,ind_2))\n",
    "        else:\n",
    "            final.append((temp[0], temp[-1]))\n",
    "            temp.clear()\n",
    "            temp.append((ind_1,ind_2))\n",
    "        last_seen = (ind_1,ind_2)\n",
    "    final.append((temp[0], temp[-1]))\n",
    "    return final\n",
    "\n",
    "def retrieve_text(tokenized_article, start_index, end_index):\n",
    "    return \" \".join([x[1] for x in tokenized_article[start_index:end_index+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_sequence = align_sequences(fr_grams,it_grams)\n",
    "glued_sequence = glue_sequence(matching_sequence, 5)\n",
    "glued_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arms they paffent the Rhine towards the year 420 crazy the one of silver to 50 lb are found unhappy Phiaramond \u001b[31mPhiaramond their leader the first\u001b[0m first king of this ferment distributed as so were the riches of monarchy The Netherlands Picardy were their Rome at the\n",
      "species pro weapons they they passed the Rhine towards the an 420 putting reward to those who had ten under Faramondo \u001b[31mFaramondo their leader and the\u001b[0m the first king sons it would be state very much better go back at of this monarchy The Countries bass and\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def display_match(match_indexes,treated_1, treated_2, padding):\n",
    "    # print(sequence)\n",
    "    start_1,end_1 = match_indexes[0][0] , match_indexes[1][0] +2\n",
    "    start_2,end_2 = match_indexes[0][1] , match_indexes[1][1] +2\n",
    "    text_1 = [retrieve_text(treated_1,start_1-padding, start_1), colored(retrieve_text(treated_1,start_1,end_1),\"red\"),retrieve_text(treated_1, end_1, end_1 + padding)]\n",
    "    text_2 = [retrieve_text(treated_2,start_2-padding, start_2), colored(retrieve_text(treated_2,start_2,end_2),\"red\"),retrieve_text(treated_2, end_2, end_2 + padding)]\n",
    "    print(*text_1)\n",
    "    print(*text_2)\n",
    "\n",
    "\n",
    "display_match(glued_sequence[23], treated_fr,treated_it, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Switzerland'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = final[0]\n",
    "retrieve_text(treated_fr,indexes[0][0], indexes[1][0])\n",
    "retrieve_text(treated_it,indexes[0][1], indexes[1][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are more manderies of the order of Malta two hundred faith of a fiècle in 1666 \u001b[31m1666 of this inveterate evil believed\u001b[0m believed xante thousand clerics secular Where regular encourage the propagation of the species in proLe county\n",
      "permit the practice of religion Reformed It s the parts go up at the top point \u001b[31mpoint similar to oldest of the\u001b[0m the kingdoms of Europe Its dark rivers which in their run lose their date dates back\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def retrieve_text(tokenized_article, start_index, end_index):\n",
    "    return \" \".join([x[1] for x in tokenized_article[start_index:end_index+1]])\n",
    "def to_ngram(index_tup):\n",
    "    return tuple([tup[1] for tup in index_tup])\n",
    "def isMatching(tup_1,tup_2):\n",
    "    return to_ngram(tup_1) == to_ngram(tup_2)\n",
    "\n",
    "def glue_sequence(index_sequence, gap_tolerance):\n",
    "    final = []\n",
    "    temp = [index_sequence[0]]\n",
    "    last_seen = index_sequence[0]\n",
    "    for index in index_sequence[1:]:\n",
    "        if index - last_seen < gap_tolerance:\n",
    "            temp.append(index)\n",
    "        else:\n",
    "            final.append((temp[0], temp[-1]))\n",
    "            temp.clear()\n",
    "            temp.append(index)\n",
    "        last_seen = index\n",
    "    final.append((temp[0], temp[-1]))\n",
    "    return final\n",
    "            \n",
    "\n",
    "def find_matching_ngrams(art_1, art_2, gap_tolerance):\n",
    "    # Probably asymmetric , let's first do article 1 \n",
    "    art_1_matching_indexes = []\n",
    "    art_2_matching_indexes = []\n",
    "    all_matching_tuples = []\n",
    "\n",
    "    for index_1,tup_1 in enumerate(art_1):\n",
    "        for index_2, tup_2 in enumerate(art_2):\n",
    "            if isMatching(tup_1,tup_2):\n",
    "                art_1_matching_indexes.append(index_1)\n",
    "                art_2_matching_indexes.append(index_2)\n",
    "                all_matching_tuples.append(tup_1)\n",
    "\n",
    "    glued_art_1_indexes = glue_sequence(sorted(art_1_matching_indexes), gap_tolerance)\n",
    "    glued_art_2_indexes = glue_sequence(sorted(art_2_matching_indexes), gap_tolerance)\n",
    "    \n",
    "\n",
    "    return glued_art_1_indexes,glued_art_2_indexes, all_matching_tuples\n",
    "\n",
    "def from_art_id_to_treated_id(indexes,article):\n",
    "    start_tuple = article[indexes[0]]\n",
    "    start_id = start_tuple[0][0]\n",
    "    end_tuple = article[indexes[1]]\n",
    "    end_id = end_tuple[-1][0]\n",
    "    return start_id,end_id\n",
    "\n",
    "def align_sequences(art_1,art_2, glued_1, glued_2):\n",
    "    matching_sequences = []\n",
    "    for id_1, seq_1 in enumerate(glued_1):\n",
    "        for id_2, seq_2 in enumerate(glued_2):\n",
    "            seq_1_gram = to_ngram(art_1[seq_1[0]])\n",
    "            seq_2_gram = to_ngram(art_2[seq_2[0]])\n",
    "            if seq_1_gram == seq_2_gram:\n",
    "                matching_sequences.append((seq_1, seq_2))\n",
    "    return matching_sequences\n",
    "\n",
    "sequences = align_sequences(fr_article,it_article,glued_fr,glued_it)\n",
    "\n",
    "indexes = sequences[0][1]\n",
    "fr_article\n",
    "\n",
    "start_index, end_index = from_art_id_to_treated_id(indexes,it_article)\n",
    "\n",
    "retrieve_text(treated_it,start_index,end_index )\n",
    "\n",
    "def display_sequence(sequence,treated_1, treated_2, art_1, art_2, padding):\n",
    "    # print(sequence)\n",
    "    start_1,end_1 = from_art_id_to_treated_id(sequence[0], art_1)\n",
    "    start_2,end_2 = from_art_id_to_treated_id(sequence[1], art_2)\n",
    "    text_1 = [retrieve_text(treated_1,start_1-padding, start_1), colored(retrieve_text(treated_1,start_1,end_1),\"red\"),retrieve_text(treated_1, end_1, end_1 + padding)]\n",
    "    text_2 = [retrieve_text(treated_2,start_2-padding, start_2), colored(retrieve_text(treated_2,start_2,end_2),\"red\"),retrieve_text(treated_2, end_2, end_2 + padding)]\n",
    "    print(*text_1)\n",
    "    print(*text_2)\n",
    "\n",
    "display_sequence(sequences[1], treated_fr, treated_it, fr_article, it_article, 15)\n",
    "\n",
    "# [retrieve_text(it_article, *x[1]) for x in sequences]\n",
    "# id = 4\n",
    "# glued_fr[id][0], fr_article[glued_fr[id][0]] , matching_tuples[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSalut la mif\u001b[0m \u001b[34mComment ça va bien\u001b[0m oklm on est la frr\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text1 = colored(\"Salut la mif\", 'red')\n",
    "text2 = colored(\"Comment ça va bien\", \"blue\")\n",
    "text3 = colored(\"oklm on est la frr\")\n",
    "print(text1, text2,text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_article\n",
    "it_article"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c08fb7d0681ab80e3191df3cdd3d9ae56e033f792b7fa01572cc6446116ecae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
