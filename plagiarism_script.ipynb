{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from helpers import correct_token, generate_n_gram, to_ngram, align_sequences, glue_sequence, display_match\n",
    "from nltk import word_tokenize\n",
    "from copy import deepcopy\n",
    "import pandas as pd \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "SC = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model parameters\n",
    "ENG_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "N_GRAM = 3\n",
    "GAP_TOLERANCE = 5 \n",
    "PADDING = 20\n",
    "\n",
    "\n",
    "def treat_article(article_path:str, context, stopwords, n):\n",
    "    with open(article_path, mode = \"r\", encoding = \"utf-8\") as f:\n",
    "        data = ''.join(f.readlines())\n",
    "    full_article = ''.join([c for c in data if c.isalnum() or c == \" \"])\n",
    "    tokenized_article = list(enumerate(word_tokenize(full_article)))\n",
    "    filtered_article = [(index,token.lower()) for index,token in tokenized_article if token not in stopwords ]\n",
    "    filtered_indexes = [index for index,_ in filtered_article]\n",
    "    corrected_article = context.parallelize(filtered_article).map(lambda x: x[1]).map(correct_token).collect()\n",
    "    corrected_article = list(zip(filtered_indexes, corrected_article))\n",
    "    n_grams = list(generate_n_gram(corrected_article, n))\n",
    "    n_gram_dict =defaultdict(list)\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_dict[to_ngram(n_gram)].append(n_gram[0][0])\n",
    "    return tokenized_article, n_gram_dict\n",
    "\n",
    "def compute_plagiarism(art_1_path: str, art_2_path: str):\n",
    "\n",
    "    # First treat the articles \n",
    "    treated_1, grams_1 = treat_article(art_1_path, SC, ENG_STOPWORDS, N_GRAM)\n",
    "    treated_2, grams_2 = treat_article(art_2_path, SC, ENG_STOPWORDS, N_GRAM)\n",
    "\n",
    "    # Align sequence and glue the sequences \n",
    "    matching_sequence = align_sequences(grams_1,grams_2)\n",
    "    glued_sequence = glue_sequence(matching_sequence, GAP_TOLERANCE)\n",
    "\n",
    "    # Create the viewer function \n",
    "    match_viewer = lambda i: display_match(glued_sequence[i], treated_1,treated_2, PADDING)\n",
    "\n",
    "    # compute the plagiarism score from both articles\n",
    "    score = 2*len(matching_sequence)/(len(treated_1) + len(treated_2)) * 100\n",
    "\n",
    "    print(\"The two articles have a similarity score of {:.2f}, with {} matching n-gram. You can use the viewer to visualize the matching sequences\".format(score, len(matching_sequence)))\n",
    "    return match_viewer, score\n",
    "\n",
    "def compute_plagiarism_from_articles(art_1, art_2):\n",
    "\n",
    "    # First treat the articles \n",
    "    treated_1, grams_1 = deepcopy(art_1)\n",
    "    treated_2, grams_2 = deepcopy(art_2)\n",
    "\n",
    "    # Align sequence and glue the sequences \n",
    "    matching_sequence = align_sequences(grams_1,grams_2)\n",
    "    glued_sequence = glue_sequence(matching_sequence, GAP_TOLERANCE)\n",
    "\n",
    "    # Create the viewer function \n",
    "    match_viewer = lambda i: display_match(glued_sequence[i], treated_1,treated_2, PADDING)\n",
    "\n",
    "    # compute the plagiarism score from both articles\n",
    "    score = 2*len(matching_sequence)/(len(grams_1) + len(grams_2)) * 100\n",
    "\n",
    "    print(\"The two articles have a similarity score of {:.2f}, with {} matching n-gram. You can use the viewer to visualize the matching sequences\".format(score, len(matching_sequence)))\n",
    "    return match_viewer, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File paths \n",
    "fr_path = \"./txt files/french.txt\"\n",
    "en_path = \"./txt files/english.txt\"\n",
    "it_path = \"./txt files/italian.txt\"\n",
    "es_path = \"./txt files/spanish.txt\"\n",
    "\n",
    "fr_article = treat_article(fr_path,SC,ENG_STOPWORDS,N_GRAM)\n",
    "en_article = treat_article(en_path,SC,ENG_STOPWORDS,N_GRAM)\n",
    "it_article = treat_article(it_path,SC,ENG_STOPWORDS,N_GRAM)\n",
    "es_article = treat_article(es_path,SC,ENG_STOPWORDS,N_GRAM)\n",
    "\n",
    "languages = [\"french\",\"english\",\"italian\",\"spanish\"]\n",
    "articles = [fr_article,en_article,it_article,es_article]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two articles have a similarity score of 0.02, with 3 matching n-gram. You can use the viewer to visualize the matching sequences\n",
      "The two articles have a similarity score of 2.35, with 81 matching n-gram. You can use the viewer to visualize the matching sequences\n",
      "The two articles have a similarity score of 3.98, with 124 matching n-gram. You can use the viewer to visualize the matching sequences\n",
      "The two articles have a similarity score of 0.01, with 2 matching n-gram. You can use the viewer to visualize the matching sequences\n",
      "The two articles have a similarity score of 0.02, with 3 matching n-gram. You can use the viewer to visualize the matching sequences\n",
      "The two articles have a similarity score of 2.97, with 100 matching n-gram. You can use the viewer to visualize the matching sequences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french</th>\n",
       "      <th>english</th>\n",
       "      <th>italian</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english</th>\n",
       "      <td>0.015613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>2.35021</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>3.982656</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>2.973536</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           french   english   italian spanish\n",
       "french        NaN       NaN       NaN     NaN\n",
       "english  0.015613       NaN       NaN     NaN\n",
       "italian   2.35021  0.010275       NaN     NaN\n",
       "spanish  3.982656  0.015681  2.973536     NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = languages,index = languages)\n",
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        _, sim = compute_plagiarism_from_articles(articles[i], articles[j])\n",
    "        results[languages[i]][languages[j]] = sim\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.98265617472298"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.min().min()\n",
    "results.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500889731993947\n",
      "french english 0.0156128024980484\n",
      "french italian 2.350210358334542\n",
      "french spanish 3.98265617472298\n",
      "english italian 0.010275116237252434\n",
      "english spanish 0.015680945038287642\n",
      "italian spanish 2.973535533749628\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "lat": [
          46,
          52.35
         ],
         "line": {
          "color": "rgb(0.3426433608617162, 0.3426433608617162, 0.3426433608617162)"
         },
         "lon": [
          2,
          -1.17
         ],
         "mode": "markers+lines",
         "name": "french - english sim : 0.02",
         "type": "scattergeo"
        },
        {
         "lat": [
          46,
          42.23
         ],
         "line": {
          "color": "rgb(150.20801830181537, 150.20801830181537, 150.20801830181537)"
         },
         "lon": [
          2,
          13.57
         ],
         "mode": "markers+lines",
         "name": "french - italian sim : 2.35",
         "type": "scattergeo"
        },
        {
         "lat": [
          46,
          40.46
         ],
         "line": {
          "color": "rgb(255.0, 255.0, 255.0)"
         },
         "lon": [
          2,
          -3.75
         ],
         "mode": "markers+lines",
         "name": "french - spanish sim : 3.98",
         "type": "scattergeo"
        },
        {
         "lat": [
          52.35,
          42.23
         ],
         "line": {
          "color": "rgb(0.0, 0.0, 0.0)"
         },
         "lon": [
          -1.17,
          13.57
         ],
         "mode": "markers+lines",
         "name": "english - italian sim : 0.01",
         "type": "scattergeo"
        },
        {
         "lat": [
          52.35,
          40.46
         ],
         "line": {
          "color": "rgb(0.3470176511186612, 0.3470176511186612, 0.3470176511186612)"
         },
         "lon": [
          -1.17,
          -3.75
         ],
         "mode": "markers+lines",
         "name": "english - spanish sim : 0.02",
         "type": "scattergeo"
        },
        {
         "lat": [
          42.23,
          40.46
         ],
         "line": {
          "color": "rgb(190.22127921275072, 190.22127921275072, 190.22127921275072)"
         },
         "lon": [
          13.57,
          -3.75
         ],
         "mode": "markers+lines",
         "name": "italian - spanish sim : 2.97",
         "type": "scattergeo"
        }
       ],
       "layout": {
        "geo": {
         "landcolor": "rgb(204, 204, 204)",
         "lataxis": {
          "dtick": 10,
          "range": [
           35,
           58
          ]
         },
         "lonaxis": {
          "dtick": 20,
          "range": [
           -15,
           20
          ]
         },
         "projection": {
          "type": "equirectangular"
         },
         "resolution": 50,
         "showland": true
        },
        "height": 600,
        "legend": {
         "orientation": "h"
        },
        "showlegend": true,
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Similarity map between articles of encyclopedias"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "france_lat_lon = (46.00,2.00)\n",
    "spain_lat_lon = (40.46,-3.75)\n",
    "italy_lat_lon = (42.23,13.57)\n",
    "england_lat_lon = (52.35,-1.17)\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "from scipy.interpolate import interp1d\n",
    "def to_rgb(hex:str):\n",
    "    x = hex[1:]\n",
    "    return tuple(int(x[i:i+2], 16) for i in (0, 2, 4))\n",
    "interpolate = interp1d([results.min().min(), results.max().max()], [0,1])\n",
    "print(interpolate(2))\n",
    "colorscales = px.colors.sequential.Viridis\n",
    "fig = go.Figure()\n",
    "test = list(zip(languages,[france_lat_lon,england_lat_lon,italy_lat_lon,spain_lat_lon]))\n",
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        first_name, first_lat_lon = test[i]\n",
    "        second_name, second_lat_lon = test[j]\n",
    "        print(first_name, second_name,results[first_name][second_name] )\n",
    "        # color = plotly.colors.find_intermediate_color(to_rgb(colorscales[3]),to_rgb(colorscales[0]),interpolate(results[first_name][second_name]))\n",
    "        color = plotly.colors.find_intermediate_color((0,0,0),(255,255,255),interpolate(results[first_name][second_name]))\n",
    "        fig.add_trace(\n",
    "            go.Scattergeo(\n",
    "                mode = \"markers+lines\",\n",
    "                lon = [first_lat_lon[1],second_lat_lon[1]],\n",
    "                lat = [first_lat_lon[0],second_lat_lon[0]],\n",
    "                line_color = 'rgb'+str(color),\n",
    "                name = first_name + \" - \"+second_name +\" sim : \"  + \"{:.2f}\".format(results[first_name][second_name])\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Similarity map between articles of encyclopedias',\n",
    "    showlegend = True,\n",
    "    geo = dict(\n",
    "        resolution = 50,\n",
    "        showland = True,\n",
    "        # showlakes = True,\n",
    "        landcolor = 'rgb(204, 204, 204)',\n",
    "        # countrycolor = 'rgb(204, 204, 204)',\n",
    "        # lakecolor = 'rgb(255, 255, 255)',\n",
    "        projection_type = \"equirectangular\",\n",
    "        # coastlinewidth = 2,\n",
    "        lataxis = dict(\n",
    "            range = [35, 58],\n",
    "            # showgrid = True,\n",
    "            dtick = 10\n",
    "        ),\n",
    "        lonaxis = dict(\n",
    "            range = [-15, 20],\n",
    "            # showgrid = True,\n",
    "            dtick = 20\n",
    "        ),\n",
    "    )\n",
    ")   \n",
    "fig.update_layout(template = \"none\", width = 800, height = 600, legend_orientation = \"h\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49, 104, 142), (68, 1, 84))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_rgb(colorscales[3]),to_rgb(colorscales[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('elena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a6dfac6a863ce74490bf67068b844c1f481ab82347803ea647cf3fdeb1d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
